[
{
	"uri": "https://devsecops-workshop.github.io/1-intro/",
	"title": "The DevSecOps Workshop",
	"tags": [],
	"description": "",
	"content": "Intro This workshop is meant to introduce you to the application development cycle leveraging OpenShift\u0026rsquo;s tooling \u0026amp; features with a special focus on securing your environment using Advanced Cluster Security for Kubernetes (ACS). And all in a fun way.\nThis is the storyline you\u0026rsquo;ll follow today:\n Create application using CodeReady Workspaces Inner Dev Loop  Use odo to create, push, change apps on the fly   Outer Dev Loop  Learn to work with Tekton Pipelines Use GitOps with ArgoCD   Secure your app and OpenShift cluster with ACS  Introduction to ACS Example use cases Add ACS scanning to Tekton Pipeline    What to Expect We try to balance guided workshop steps and challenging you to use your knowledge to learn new skills. This means you\u0026rsquo;ll get detailed step-by-step instructions for every new chapter/task, later on the guide will become less verbose and we\u0026rsquo;ll weave in some challenges.\nA good understanding of how OpenShift works together with hands-on experience is expected. For example we will not tell you how to log in oc to your cluster or tell you what it is\u0026hellip; ;)\n Workshop Environment You will be provided with freshly installed OpenShift 4 clusters running in AWS. Depending on attendee numbers we might ask you to gather in teams. Some workshop tasks must be done only once for the cluster (e.g. installing Operators), others like deploying and securing the application can be done by every team member separately in her/his own Project. This will be mentioned in the guide.\nWorkshop Flow We\u0026rsquo;ll tackle the topics at hand step by step with an introduction covering the things worked on before every section.\n"
},
{
	"uri": "https://devsecops-workshop.github.io/2-prepare-cluster/",
	"title": "Prepare Cluster",
	"tags": [],
	"description": "",
	"content": "Cluster Preparation Before you start you have to install a number of components you\u0026rsquo;ll use during the workshop. The first two are Gitea for providing Git services in your cluster and CodeReady Workspaces as development environment. But fear not, both are managed by an operator.\nInstall and Prepare Gitea We\u0026rsquo;ll need Git repository srvices, so let\u0026rsquo;s just install trusted Gitea using an operator:\nThis is a good example of how you can integrate an operator into your catalog that is not part of the default OperatorHub already.\n  Log in the oc command to your OpenShift cluster (the easiest way is to use the Copy login command link in the OCP web UI). Now using oc add the Gitea Operator to your OperatorHub  oc apply -f https://raw.githubusercontent.com/redhat-gpte-devopsautomation/gitea-operator/master/catalog_source.yaml  Go to OperatorHub and search for Gitea Install the Gitea Operator with default settings Create a new project git Go to Installed Operators -\u0026gt; Gitea Operator and click Create instance in the new project. On the Create Gitea page switch to the YAML view and make sure the following spec values exist:  spec: giteaSsl: true giteaAdminUser: gitea giteaAdminPassword: \u0026quot;gitea\u0026quot; giteaAdminEmail: opentlc-mgr@redhat.com  Click Create  After creation has finished:\n Look up and access the repository route, this will take you to the Gitea web UI Login to Gitea with user gitea and password gitea Clone the example repo:  Click the + dropdown and choose New Migration As type choose Git URL: https://github.com/devsecops-workshop/quarkus-build-options.git Click Migrate Repository    In the cloned repository you\u0026rsquo;ll find a devfile.yml for use with CodeReady Workspaces (CRW). You have to edit the file to point to your own repository:\n Copy the clone URL of your repo. Edit the devfile.yml by clicking it and selecting the Edit File pencil icon.  In the file change location: to the clone URL you just copied. Commit changes    Install and Prepare CodeReady Workspaces (CRW)  Install the CRW Operator from OperatorHub (not the Tech Preview one!) using default settings. Go to Installed Operators -\u0026gt; CodeReady Workspaces and create a new instance (CodeReady Workspaces instance Specification) using the default settings. Wait until deployment has finished.  As you\u0026rsquo;ll start your CRW workspace using the devfile mechanism, you have to compose the URL from the codeready route and the devfile location.\n First look up the codeready route and the full URL to access the devfile.yml in Gitea by using the \u0026ldquo;raw\u0026rdquo; format button. Now use both components to build an URL like below:  https://\u0026lt;codeready-openshift-route\u0026gt;/f?url=\u0026lt;full URL to devfile.yml\u0026gt; For example it could look like this:\nhttps://\u0026lt;codeready-openshift-workspaces.apps.cluster-mqpmq.mqpmq.sandbox283.opentlc.com/f?url=https://repository-git.apps.cluster-mqpmq.mqpmq.sandbox283.opentlc.com/gitea/quarkus-build-options/raw/branch/master/devfile.yml  Now use this URL to launch your CRW Workspace in your Browser  Log in as User opentlc-mgr Apply permissions In the next form add an email address, it will be added to Keycloak    Have a good look around the UI, it should look familiar if you have ever worked with VSCode and the like. To prepare the Workspace for developing with OpenShift, ru the following steps:\n In the CRW shortcuts menu to the right (\u0026ldquo;cube icon\u0026rdquo;)  Run install oc Run install odo   Close the tabs the installation opened.  "
},
{
	"uri": "https://devsecops-workshop.github.io/3-inner-loop/",
	"title": "Inner Loop",
	"tags": [],
	"description": "",
	"content": "In this part of the workshop you\u0026rsquo;ll experience how modern software development using the OpenShift tooling can be done in a fast, iterative way. Inner loop here means this is the way, sorry, process, for a developer to try out new things and quickly change and test her/his code on OpenShift without having to build new images all the time or being a Kubernetes expert.\nAs an example you\u0026rsquo;ll create a new Quarkus application. You don\u0026rsquo;t need to have prior experience programming in Java as this will be kept really simple.\n Bring up your CodeReady Workspace in your browser. In CRW open Terminal  In My Workspace (cube icon) to the right click New Terminal   Copy the oc login command from your OpenShift cluster (tope right \u0026gt; Username \u0026gt; Copy login command), and use it to log in oc in the CRW terminal. Create a new project deepspace-dev  ./oc new-project deepspace-dev  First use odo (\u0026ldquo;OpenShift Do\u0026rdquo;) to list the programming languages/frameworks it supports  Note that not all languages odo works with are officially supported!    ./odo catalog list components Now initialize a new Quarkus application\n./odo create java-quarkus Make the app accessible via http\n./odo url create deepdive-app And finally push the app to OpenShift\n./odo push To test the app:\n In OpenShift open the deepspace-dev project and switch to the Developer Console Open the Topology tab and click on the top right link of OpenShift icon to display the website of the app Your app should show up as a simple web page. In the RESTEasy JAX-RS section click the @Path endpoint /hello to see the result.  Now for the fun part: Using odo you can just dynamically change your code and push it out again without doing a new build! No dev magic involved:\n In your CRW Workspace on the left expand the file tree to open file src/main/java/org/acmeGreetingRessource.java to Hello Deepdive Push the code to OpenShift again  ./odo push  And reload the app webpage. The change should be there in a matter of seconds  "
},
{
	"uri": "https://devsecops-workshop.github.io/4-outer-loop/",
	"title": "Outer Loop",
	"tags": [],
	"description": "",
	"content": "Now that you have seen how a developer can quickly start to code using modern tooling, it\u0026rsquo;s time to learn how to proceed with the application to production. The first step is to implement a build pipeline to automate new builds. Let\u0026rsquo;s call this stage int for integration.\nTo create and run the build pipeline you\u0026rsquo;ll use OpenShift Pipelines (Tekton). The first step is to install it:\n Install OpenShift Pipelines from OperatorHub  After this create a new deployment of your game-changing application:\n Switch to the OpenShift Developer Console Create a new project deepspace-int Click the +Add menu entry to the right and choose From Git As Git Repo URL enter your Gitea clone URL (There my may be warning about the repo url that you can ignore) As Builder Image keep or select Red Hat OpenJDK 11 (RHEL 7) Remove the segment -git from Application and Name Check Add pipeline Click Create In the main menu left, click in Pipeline and observe how the Tekton Pipeline is created  Now your build pipeline has been set up and is ready to run. There is one more step in preparation of the security part of this workshop. We need a way to build and deploy from an older image with some security issues in it.\n In the default Java image stream create another Image Stream Tag that points to an older version. Using the Administrator view, switch to the project openshift and under Builds access the ImageStreams Search and open the ImageStream Java Switch to YAML view and add the following snippet to the tags: section.  Be careful to keep the needed indents!    - name: java-old-image annotations: description: Build and run Java applications using Maven and OpenJDK 8. iconClass: icon-rh-openjdk openshift.io/display-name: Red Hat OpenJDK 8 (UBI 8) sampleContextDir: undertow-servlet sampleRepo: \u0026#39;https://github.com/jboss-openshift/openshift-quickstarts\u0026#39; supports: \u0026#39;java:8,java\u0026#39; tags: \u0026#39;builder,java,openjdk\u0026#39; version: \u0026#39;8\u0026#39; from: kind: DockerImage name: \u0026#39;registry.redhat.io/openjdk/openjdk-11-rhel7:1.1-9\u0026#39; generation: 4 importPolicy: {} referencePolicy: type: Local This will add a tag java-old-image that points to an older version. The image can be inspected here: https://catalog.redhat.com/software/containers/openjdk/openjdk-11-rhel7/5bf57185dd19c775cddc4ce5?tag=1.10-1.1630314161\u0026amp;push_date=1630540002000\u0026amp;container-tabs=security\n Have a look at version 1.1-9  Now run the pipeline again. Open your quarkus-build-options pipeline and choose \u0026ldquo;Run\u0026rdquo; from the pull down menu of the \u0026ldquo;Actions\u0026rdquo; button on the upper right hand side. Edit the \u0026ldquo;VERSION\u0026rdquo; parameter and use \u0026ldquo;java-old-image\u0026rdquo; tag.\n"
},
{
	"uri": "https://devsecops-workshop.github.io/5-gitops/",
	"title": "Configure GitOps",
	"tags": [],
	"description": "",
	"content": " Install GitOps Operator from OperatorHub Clone the Config GitOps Repo to Gitea https://github.com/devsecops-workshop/openshift-gitops-getting-started.git Create OpenShift Project deepspace-prod Give ArgoCD Permissions to create objects in namespace deepspace-prod   oc adm policy add-role-to-user admin system:serviceaccount:openshift-gitops:openshift-gitops-argocd-application-controller -n deepspace-prod  Give namespace deepspace-prod permissions to pull images from deepspace-int  oc policy add-role-to-user \\ system:image-puller system:serviceaccount:deepspace-prod:default \\ --namespace=deepspace-int  Go to ArgoCD URL (The is new shortcut at the top right menu with the squares) User is admin and password will be in namespace openshift-gitops in Secret openshift-gitops-cluster Create App  Application name : quarkus-options Project : default Self Heal true Repo URL : https://repository-gitea.apps.{YOUR DOMAIN}.com/gitea/openshift-gitops-getting-started.git Path : environments/dev Cluster URL : https://kubernetes.default.svc Namespace : deepspace-prod   Watch the resources (Deployment, Service, Route) get rolled out to the namespace deepspace-prod Add a new custom Tekton task (Switch to Administrator Perspective \u0026gt; Pipelines \u0026gt; Tasks \u0026gt; New Task) that can push to a git repo. Make sure to replace {YOUR_DOMAIN}  apiVersion: tekton.dev/v1beta1 kind: Task metadata: annotations: tekton.dev/pipelines.minVersion: 0.12.1 tekton.dev/tags: git name: git-update-deployment namespace: deepspace-int labels: app.kubernetes.io/version: \u0026#39;0.1\u0026#39; operator.tekton.dev/provider-type: community spec: description: This Task can be used to update image digest in a Git repo using kustomize params: - name: GIT_REPOSITORY type: string - name: GIT_USERNAME type: string - name: GIT_PASSWORD type: string - name: CURRENT_IMAGE type: string - name: NEW_IMAGE type: string - name: NEW_DIGEST type: string - name: KUSTOMIZATION_PATH type: string results: - description: The commit SHA name: commit steps: - image: \u0026#39;docker.io/alpine/git:v2.26.2\u0026#39; name: git-clone resources: {} script: | rm -rf git-update-digest-workdir git clone $(params.GIT_REPOSITORY) git-update-digest-workdir workingDir: $(workspaces.workspace.path) - image: \u0026#39;quay.io/wpernath/kustomize-ubi:latest\u0026#39; name: update-digest resources: {} script: \u0026gt; #!/usr/bin/env bash echo \u0026#34;Start\u0026#34; pwd cd git-update-digest-workdir/$(params.KUSTOMIZATION_PATH) pwd #echo \u0026#34;kustomize edit set image #$(params.CURRENT_IMAGE)=$(params.NEW_IMAGE)@$(params.NEW_DIGEST)\u0026#34; kustomize version kustomize edit set image $(params.CURRENT_IMAGE)=$(params.NEW_IMAGE)@$(params.NEW_DIGEST) echo \u0026#34;##########################\u0026#34; echo \u0026#34;### kustomization.yaml ###\u0026#34; echo \u0026#34;##########################\u0026#34; ls cat kustomization.yaml workingDir: $(workspaces.workspace.path) - image: \u0026#39;docker.io/alpine/git:v2.26.2\u0026#39; name: git-commit resources: {} script: \u0026gt; pwd cd git-update-digest-workdir git config user.email \u0026#34;tekton-pipelines-ci@redhat.com\u0026#34; git config user.name \u0026#34;tekton-pipelines-ci\u0026#34; git status git add $(params.KUSTOMIZATION_PATH)/kustomization.yaml # git commit -m \u0026#34;[$(context.pipelineRun.name)] Image digest updated\u0026#34; git status git commit -m \u0026#34;[ci] Image digest updated\u0026#34; git status git push RESULT_SHA=\u0026#34;$(git rev-parse HEAD | tr -d \u0026#39;\\n\u0026#39;)\u0026#34; EXIT_CODE=\u0026#34;$?\u0026#34; if [ \u0026#34;$EXIT_CODE\u0026#34; != 0 ] then exit $EXIT_CODE fi # Make sure we don\u0026#39;t add a trailing newline to the result! echo -n \u0026#34;$RESULT_SHA\u0026#34; \u0026gt; $(results.commit.path) workingDir: $(workspaces.workspace.path) workspaces: - description: The workspace consisting of maven project. name: workspace  Add this Task to your Pipeline by adding it to the YAML like this Make sure to replace {YOUR_DOMAIN}  ... - name: git-update-deployment params: - name: GIT_REPOSITORY value: \u0026gt;- https://repository-gitea.apps.{YOUR DOMAIN}/gitea/openshift-gitops-getting-started.git - name: GIT_USERNAME value: gitea - name: GIT_PASSWORD value: gitea - name: CURRENT_IMAGE value: \u0026gt;- image-registry.openshift-image-registry.svc:5000/deepspace-int/quarkus-build:latest - name: NEW_IMAGE value: \u0026gt;- image-registry.openshift-image-registry.svc:5000/deepspace-int/quarkus-build - name: NEW_DIGEST value: $(tasks.build.results.IMAGE_DIGEST) - name: KUSTOMIZATION_PATH value: environments/dev runAfter: - build taskRef: kind: Task name: git-update-deployment workspaces: - name: workspace workspace: workspace  Create a secret with credentails for your gitea repository  kind: Secret apiVersion: v1 metadata: name: gitea namespace: deepspace-int annotations: tekton.dev/git-0: \u0026#39;https://repository-git.apps.{YOUR DOMAIN}\u0026#39; data: password: Z2l0ZWE= username: Z2l0ZWE= type: kubernetes.io/basic-auth  Edit the serviceaccount \u0026ldquo;pipeline\u0026rdquo; and add the secret to it:  kind: ServiceAccount apiVersion: v1 metadata: name: pipeline namespace: deepspace-int secrets: - name: gitea  Run the pipeline and see that in your Gitea repo /environment/dev/kustomize.yaml is updated with the newImage version This will tell ArgoCD to update the Deployment with this new Image version Check that the new image is rolled out (you may need to Sync manually in ArgoCD to speed it up)  "
},
{
	"uri": "https://devsecops-workshop.github.io/10-rhacs-setup/",
	"title": "Install and Configure ACS",
	"tags": [],
	"description": "",
	"content": "During the workshop you went through the OpenShift developer experience starting from software development using Quarkus and odo, moving on to automating build and deployment using Tekton pipelines and finally using GitOps for production deployments.\nNow it\u0026rsquo;s time to add another extremely important piece to the setup; enhancing application security in a conainerized world. Using the most recent addition to the OpenShift portfolio: Red Hat Advanced Cluster Security for Kubernetes!\nInstall RHACS Install the Operator  Install the \u0026ldquo;Advanced Cluster Security for Kubernetes\u0026rdquo; operator from OperatorHub with the default values.  Install the main component Central  Navigate to Operators → Installed Operators Select the ACS operator You are probably in the openshift-operator project, create a new project stackrox (use the Project drop-down) Select Provided APIs → Central → Create Central Change the name if you like and accept the default values Click Create Central After deployment has finished (Status \u0026ldquo;Conditions: Deployed, Initialized\u0026rdquo;) access the RHACS Portal to verify the installation  Look up the central-htpasswd secret that was created to get the password Look up and access the route central which was also generated automatically.   This will get you to the RHACS portal, accept the self-signed certificate and login as user admin with the password from the secret.  Now you have a Central instance that provides the following services in an RHACS setup:\n  The application management interface and services. It handles data persistence, API interactions, and user interface access. You can use the same Central instance to secure multiple OpenShift or Kubernetes clusters.\n  Scanner, which is a vulnerability scanner for scanning container images. It analyzes all image layers to check known vulnerabilities from the Common Vulnerabilities and Exposures (CVEs) list. Scanner also identifies vulnerabilities in packages installed by package managers and in dependencies for multiple programming languages.\n  To actually do and see anything you need to add a SecuredCluster (be it the same or another OpenShift cluster). For effect go to the RHACS Portal, the Dashboard should by pretty empty, click on the Compliance link in the menu to the left, lots of zero\u0026rsquo;s and empty panels, too.\nBecause you don\u0026rsquo;t have a monitored and secured OpenShift cluster yet.\nPrepare to add Secured Clusters First you have to generate an init bundle which contains certificates and is used to authenticate a SecuredCluster to the Central instance, again regardless if it\u0026rsquo;s the same cluster as the Central instance or a remote/other cluster.\nIn the RHACS Portal:\n Navigate to Platform Configuration → Integrations. Under the Authentication Tokens section, click on Cluster Init Bundle. Click Generate bundle Enter a name for the cluster init bundle and click Generate. Click Download Kubernetes Secret File to download the generated bundle.  The init bundle needs to be applied on all OpenShift clusters you want to secure \u0026amp; monitor.\nPrepare the Secured Cluster For this workshop we run Central and SecuredCluster on one OpenShift cluster. E.g. we monitor and secure the same cluster the central services live on.\nApply the init bundle\n Use the oc command to log in to the OpenShift cluster as cluster-admin.  The easiest way might be to use the Copy login command link from the UI   Switch to the Project you installed ACS Central in, it should be stackrox. Run oc create -f \u0026lt;init_bundle\u0026gt;.yaml -n stackrox pointing to the init bundle you downloaded from the Central instance and the Project you created. This will create a number of secrets:  secret/collector-tls created secret/sensor-tls created secret/admission-control-tls created Add the Cluster as SecuredCluster to ACS Central Now you are ready to install the SecuredClusters instance, this will deploy the secured cluster services:\n Go to the ACS Operator in Operators-\u0026gt;Installed Operators Using the Operator create an instance of the Secured Cluster type in the Project you created (should be stackrox) Change the Cluster Name for the cluster if you want, it\u0026rsquo;ll appear under this name in the RHACS Portal And most importantly for Central Endpoint enter the address and port number of your Central instance, this is the same as the RACS Portal.  If the RHACS Portal is available at https://central-stackrox.apps.cluster-65h4j.65h4j.sandbox1803.opentlc.com/ the endpoint is central-stackrox.apps.cluster-65h4j.65h4j.sandbox1803.opentlc.com:443.   Under Admission Control Settings  enable listenOnCreates and listenOnUpdates Set Contact Image Scanners to ScanIfMissing   Click Create  Now go to your RHACS Portal again, after a couple of minutes you should see you secured cluster under Platform Configuration-\u0026gt;Clusters. Wait until all Cluster Status indicators become green.\nCreate a serviceaccount to scan the internal registry To enable scanning of all images in the internal registry, you\u0026rsquo;ll have to\n add a serviceaccount assign it the needed privileges configure an Integration in ACS  Create ServiceAccount to read images from Registry\n Make sure you are in the stackrox Project User Management -\u0026gt; ServiceAccounts -\u0026gt; Create ServiceAccount Replace the example name with acs-registry-reader and click Create In the new ServiceAccount, under Secrets click the ...-token-... secret Under Data copy the Token Using oc give the ServiceAccount the right to read images from all projects:  oc adm policy add-cluster-role-to-user 'system:image-puller' -z acs-registry-reader -n stackrox or\noc adm policy add-cluster-role-to-user 'system:image-puller' system:serviceaccount:stackrox:acs-registry-reader -n stackrox And you\u0026rsquo;ll need the service address for the registry, look it up e.g. from the Environment of a registry pod.\n It should be something like: image-registry.openshift-image-registry.svc:5000  Add Registry Integration to ACS\nAccess the RHACS Portal and add an integration of type Generic Docker Registry from the Platform Configuration -\u0026gt; Integrations menu.\nFill in the fields:\n Give the integration a unique name that should include the cluster name Set Types to Registry Set Endpoint to the registry service address Put in the ServiceAccount name as username and the token you copied above Select Disable TLS certificate validation Press the test button to validate the connection and press Save when the test is successful.  You can also (this might be easier to maintain) just change the user and password for the exisintg, auto-generated entries for the entries https://image-registry.openshift-image-registry.svc:5000 and https://image-registry.openshift-image-registry.svc.cluster.local:5000.\n "
},
{
	"uri": "https://devsecops-workshop.github.io/11-rhacs-warmup/",
	"title": "Getting to know ACS",
	"tags": [],
	"description": "",
	"content": "Before we start to integrate Red Hat Advanced Cluster Security in our setup, you should become familiar with the basic concepts.\nACS Features ACS delivers on these security use cases:\n Vulnerability Management: Protect the software supply chain and prevent known vulnerabilities from being used as an entry point in your applications. Configuration Management: Leverage the OpenShift platform for declarative security to prevent or limit attacks, even in the presence of exploitable vulnerabilities. Network Segmentation: Using Kubernetes network policies in OpenShift, restrict open network paths for isolation and to prevent lateral movement by attackers. Risk Profiling: Prioritize applications and security risks automatically to focus investigation and mitigation efforts. Threat detection and incident response: Continuous observation and response in order to take action on attack-related activity, and to use observed behavior to inform mitigation efforts to harden security. Compliance: Making sure that industry and regulatory standards are being met in your OpenShift environments.  UI Overview   Dashboard: The dashboard serves as the security overview - helping the security team understand what the sources of risk are, categories of violations, and gaps in compliance. All of the elements are clickable for more information and categories are customizable.\n  Top bar: Near the top, we see an overview of our OpenShift clusters. It provides insight into the usage of images and secrets. The top bar provides links to Search, Command-line tools, Cluster Health, Documentation, API Reference, and logged-in user account\n  Left menus: The left hands side menus provide navigation into each of the security use-cases, as well as product configuration to integrate with your existing tooling.\n  Global Search: On every page throughout the UI, the global search allows you to search for any data that ACS tracks.\n  Exploring the Security Use Cases Now start to explore the Security Use Cases ACS targets as provided in the left side menu.\n  Network Graph:\n The Network Graph is a flow diagram, firewall diagram, and firewall rule builder in one. The default view Active the actual traffic for the Past Hour between the deployments in all namespaces is shown.    Violations:\n Violations record all times where a policy criteria was triggered by any of the objects in your cluster - images, components, deployments, runtime activity.    Compliance:\n The compliance reports gather information for configuration, industry standards, and best practices for container-based workloads running in OpenShift.    Vulnerability Management:\n Vulnerability Management provides several important reports - where the vulnerabilities are, which are the most widespread or the most recent, where my images are coming from. In the upper right are buttons to link to all policies, CVEs, and images, and a menu to bring you to reports by cluster, namespace, deployment, and co.    Configuration Management:\n Configuration management provides visibility into a number of infrastructure components: clusters and nodes, namespaces and deployments, and Kubernetes systems like RBAC and secrets.    Risk:\n The Risk view goes beyond the basics of vulnerabilities. It helps to understand how deployment configuration and runtime activity impact the likelihood of an exploit occurring and how successful those exploits will be. This list view shows all deployments, in all clusters and namespaces, ordered by Risk priority.    System Policies As the foundation of ACS are the system policies, have a good look around:\n Navigate to the System Policies section from Platform Configuration in the left side menu. You will get an overview of the Built-in Policies All of the policies that ship with the product are designed with the goal of providing targeted remediation that improves security hardening. You’ll see this list contains many Build and Deploy time policies to catch misconfigurations early in the pipeline, but also Runtime policies. These policies come from us at Red Hat - our expertise, our interpretation of industry best practice, and our interpretation of common compliance standards, but you can modify them or create your own.  Filters Most UI pages have a filters section at the top that allows you to narrow the reporting view to matching or non-matching criteria. Almost all of the attributes that ACS gathers are filterable, try it out:\n Go to the Risk view Click in the Filters Bar Start typing Process Name and select the Process Name key Type java and press enter; click away to get the filters dropdown to clear You should see your deployment that has been “seen” running Java since it started Try another one: limit the filter to your Project namespace only Note the Create Policy button. It can be used to create a policy from the search filter to automatically identify this criteria.  "
},
{
	"uri": "https://devsecops-workshop.github.io/12-create-policy/",
	"title": "Create a Custom Security Policy",
	"tags": [],
	"description": "",
	"content": "Objective You should by now have one or more pipelines to build your application, now we want to secure the build and deployment of it. For the sake of this workshop we\u0026rsquo;ll take a somewhat simplified use case:\nWe want to scan our application image for the Red Hat Security Advisory RHSA-2020:5566 concerning openssl-lib.\nIf this RHSA is found in an image we don\u0026rsquo;t want to deploy the application using it.\nThese are the steps you will go through:\n Create a custom Security Policy to check for the advisory Test if the policy is triggered in non-enforcing mode  with an older image version that contains the advisory and then with a newer versiowhere the issue has been fixed.   The final goal is to integrate the policy into the build pipeline  Create a Custom System Policy First create the system policy. In the ACS Portal do the following:\n System Policies-\u0026gt;Create Policy NAME: Deepspace RHSA-2020:5566 Severity: Critical Lifecycle Stages: Build, Deploy Categories: Deepspace  This will create a new Category if it doesn\u0026rsquo;t exist   -\u0026gt;Next Click Add a new condition Find the policy field CVE in Image Contents and drag-and-drop it into the Drop A Policy Field Inside area. Put RHSA-2020:5566 into the CVE field Click -\u0026gt;Next The next page will show Deployments that would generate violations Click -\u0026gt;Next On the next page you could enable enforcement of the policy for the Build and/or Deploy stages. Don\u0026rsquo;t enable enforcement yet! Click Save  Test the Policy Start the pipeline with the affected image version:\n Go to the Pipeline, Start it and set Version to java-old-image Follow the Violations in the ACS Portal Expected result:  You\u0026rsquo;ll see the build deployments (Quarkus-Build-Options-Git-Gsklhg-Build-...) come and go when they are finished. When the final build is deployed you\u0026rsquo;ll see a violation in ACS Portal for policy Deepspace RHSA-2020:5566 (Check the Time of the violation)    Now start the pipeline with the fixed image version that doesn\u0026rsquo;t contain the CVE anymore:\n Start the pipeline again but this time leave the Java Version as is. Follow the Violations in the ACS Portal Expected result:  You\u0026rsquo;ll see the build deployments come up and go When the final build is deployed you\u0026rsquo;ll see the policy violation in ACS Portal from before is gone because the image no longer contains it.    This shows how ACS is automatically scanning images when they become active against all enabled policies. But we don\u0026rsquo;t want to just see a violation after the image has been deployed, we want to disable the deployment during build time! So the next step is to integrate the check into the build pipeline and enforce it (don\u0026rsquo;t deploy the application).\n"
},
{
	"uri": "https://devsecops-workshop.github.io/13-rhacs-pipeline/",
	"title": "Integrating ACS into the Pipeline",
	"tags": [],
	"description": "",
	"content": "Finally: Putting the Sec in DevSecOps! There are basically two ways to interface with ACS. The UI, which focuses on the needs of the security team, and a separate \u0026ldquo;interface\u0026rdquo; for developers to integrate into their existing toolset (CI/CD pipeline, consoles, ticketing systems etc): The roxctl commandline tool. This way ACS provides a familiar interface to understand and address issues that the security team considers important.\n ACS policies can act during the CI/CD pipeline to identify security risk in images before they are run as a container.\nIntegrate Image Scan into the Pipeline You should have created and build a custom policy in ACS and tested it for triggering violations. Now you will integrate it into the build pipeline.\nLet\u0026rsquo;s go: Prepare roxctl Build-time policies require the use of the roxctl command-line tool which is available for download from the ACS Central UI, in the upper right corner of the dashboard. Roxctl needs to authenticate to ACS Central to do anything. It can use either username and password authentication to Central, or API token based. It\u0026rsquo;s good practice to use a token so that\u0026rsquo;s what you\u0026rsquo;ll do.\nCreate the roxctl token On the ACS portal:\n navigate to Configure \u0026gt; Integrations. Scroll down to the Authentication Tokens category, and select API Token. Click Generate Token. Enter the name pipeline for the token and select the role Admin. Select Generate Save the contents of the token somewhere!  Create OCP secret with token In your OCP cluster, create a secret with the API token in the project your pipeline lives in:\n In the UI switch to your Project Create a new key/value secret named roxsecrets Introduce these key/values into the secret:  rox_central_endpoint: \u0026lt;the URL to your ACS Portal\u0026gt;  It should be something like central-stackrox.apps.cluster-psslb.psslb.sandbox555.opentlc.com:443\u0026rdquo;   rox_api_token: \u0026lt;the API token you generated\u0026gt;    Create a Scan Task You are now ready to create a new pipeline task that will use roxctl to scan the image build in your pipeline before the deploy step:\n In the OCP UI, make sure you are still in the project with your pipeline and the secret roxsecrets Go to Pipelines-\u0026gt;Tasks Click Create-\u0026gt; ClusterTask Replace the YAML displayed with this:  apiVersion: tekton.dev/v1beta1 kind: ClusterTask metadata: name: rox-image-check spec: params: - description: \u0026gt;- Secret containing the address:port tuple for StackRox Central (example - rox.stackrox.io:443) name: rox_central_endpoint type: string - description: Secret containing the StackRox API token with CI permissions name: rox_api_token type: string - description: \u0026#39;Full name of image to scan (example -- gcr.io/rox/sample:5.0-rc1)\u0026#39; name: image type: string - description: Use image digest result from s2i-java build task name: image_digest type: string results: - description: Output of `roxctl image check` name: check_output steps: - env: - name: ROX_API_TOKEN valueFrom: secretKeyRef: key: rox_api_token name: $(params.rox_api_token) - name: ROX_CENTRAL_ENDPOINT valueFrom: secretKeyRef: key: rox_central_endpoint name: $(params.rox_central_endpoint) image: registry.access.redhat.com/ubi8/ubi-minimal:latest name: rox-image-check resources: {} script: \u0026gt; #!/usr/bin/env bash set +x curl -k -L -H \u0026#34;Authorization: Bearer $ROX_API_TOKEN\u0026#34; https://$ROX_CENTRAL_ENDPOINT/api/cli/download/roxctl-linux --output ./roxctl \u0026gt; /dev/null; echo \u0026#34;Getting roxctl\u0026#34; chmod +x ./roxctl \u0026gt; /dev/null ./roxctl image check -c Deepspace --insecure-skip-tls-verify -e $ROX_CENTRAL_ENDPOINT --image $(params.image)@$(params.image_digest) Take your time to understand the Tekton task definition:\n First some parameters are defined, it\u0026rsquo;s important to understand some of these are taken or depend on the build task that run before. The script action pulls the roxctl binary into the pipeline workspace so you\u0026rsquo;ll always have a version compatible with your ACS version. The most important bit is the roxctl execution, of course.  it executes the image check command only checks against policies from category Deepspace that was created above. This way you can check against a subset of policies! defines the image to check and it\u0026rsquo;s digest    Add the Task to the Pipeline  Now add the rox-image-check task to your pipeline between the build and deploy steps.  After you added it you have to fill in values for the parameters the task defines. Click the task, a form with the parameters will open, fill it in:\n rox_central_endpoint: roxsecrets rox_api_token: roxsecrets image: image-registry.openshift-image-registry.svc:5000/deepspace-int/quarkus-deepspace  Adapt the Project name if you changed it   image_digest: $(tasks.build.results.IMAGE_DIGEST)  This variable takes the result of the build task and uses it in the scan task.   Click Save  Test the Scan Task With our Deepspace Security Policy still not set to enforce we first are going to test the pipeline integration. Start the pipeline with Java Version java-old-test\n Expected Result:  The rox-image-check task should succeed, but if you have a look at the output (click the task in the visual representation) you should see that the build violated our policy!    To test the fixed image, just start the task with the default (latest) Java version again.\n Expected Result:  The rox-image-check task should succeed, if you have a look at the output you should see no policy violation!    Enforce the Policy The last step is to enforce the Security Policy. If the policy is violated the pipeline should be stopped and the application should not be deployed.\n Edit the Security Policy in ACS Portal and set enforcement to On for the stages Build and Deploy Run the pipeline again, first with Java Version java-old-test and then with the latest default version. Expecte results:  We are sure you know by now what to expect! The pipeline should fail with the old Java image version and succeed with the latest image version!    "
},
{
	"uri": "https://devsecops-workshop.github.io/14-runtime-security/",
	"title": "Securing Runtime Events",
	"tags": [],
	"description": "",
	"content": "So far you\u0026rsquo;ve seen how ACS can handle security issues concerning Build and Deploy stages. But ACS is also able to detect and secure container runtime behaviour. Let\u0026rsquo;s have a look\u0026hellip;\nHandling Security Issues at Runtime As a scenario let\u0026rsquo;s assume you won\u0026rsquo;t to protect container workloads against attackers who are trying to install software. ACS comes with pre-configured policies for Ubuntu and Red Hat-based containers to detect if a package management tool is installed, this can be used in the Build and Deploy stages:\n Red Hat Package Manager in Image  And, more important for this section about runtime security, a policy to detect the execution of a package manager as a runtime violation, using Kernel instrumentation:\n Red Hat Package Manager Execution  In the ACS Portal, go to Platform Configuration-\u0026gt;System Policies, search for the policies by e.g. typing policy and then red hat into the filter. Open the policy detail view by clicking it and have a look at what they do.\nYou can use the included policies as they are but you can always e.g. clone and adapt them to your needs or write completely new ones.\n As you can see the Red Hat Package Manager Execution policy will alert as soon as a process rpm or dnf or yum is executed.\nLike with most included policies it is not set to enforce!\n Test the Runtime Policy To see how the alert would look like, we have to trigger the condition:\n You should have a namespace with your Quarkus application runnning In the OCP UI navigate to the pod and open a terminal into the container Run yum search test. Or whatever. Go to the Violations view in the ACS Portal. You should see a violation of the policy, if you click it, you\u0026rsquo;ll get the details. Run several yum commands in the terminal and check back with the Violations view:  As long as you stay in the same deployment, there won\u0026rsquo;t be a new violation but you will see the details for every new violation of the same type in the details.    Enforce Runtime Protection But the real fun starts when you enforce the policy. Using the included policy, it\u0026rsquo;s easy to just \u0026ldquo;switch it on\u0026rdquo;:\n In the ACS Portal bring up the Red Hat Package Manager Execution again. Click the Edit button above the Details view to the right. Click -\u0026gt;Next until you arrive at the Enforcement Behaviour page. Set Runtime to ON Click Save  Now trigger the policy again by opening a terminal into the pod and executing yum. See what happens:\n Runtime enforcement will kill the pod immediately (via k8s). Kubernetes will scale it up again automatically  This is to be expected and allows to contain a potential compromise while not causing a production outage.    "
},
{
	"uri": "https://devsecops-workshop.github.io/100-playground/",
	"title": "Playground",
	"tags": [],
	"description": "",
	"content": "Test Using a variable from config.toml Value of var \u0026ldquo;deployment\u0026rdquo; is: production\nAnnotations: Text here\n Text here\n Fold-outs Click here for Solution     Module: yum\n  Arguments: name=nano\n  Tick Enable Privilege Escalation\n    "
},
{
	"uri": "https://devsecops-workshop.github.io/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "DevSecOps Workshop This is the documentation for the DevSecOps Workshop\n"
},
{
	"uri": "https://devsecops-workshop.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://devsecops-workshop.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]