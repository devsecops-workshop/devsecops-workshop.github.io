[
{
	"uri": "https://devsecops-workshop.github.io/1-intro/",
	"title": "The DevSecOps Workshop",
	"tags": [],
	"description": "",
	"content": "Intro This is the storyline you\u0026rsquo;ll follow:\n Create application using the browser based development environment CodeReady Workspaces Setting up the Inner Development Loop for the individual developer  Use the cli tool odo to create, push, change apps on the fly   Setting up the Outer Development Loop for the team CI/CD  Learn to work with OpenShift Pipelines based on Tekton Use OpenShift GitOps based on ArgoCD   Secure your app and OpenShift cluster with ACS  Introduction to ACS Example use cases Add ACS scanning to Tekton Pipeline    What to Expect This workshop is for intermediate OpenShift users. A good understanding of how OpenShift works along with hands-on experience is expected. For example we will not tell you how to log in with oc to your cluster or tell you what it is\u0026hellip; ;)\n We try to balance guided workshop steps and challenging you to use your knowledge to learn new skills. This means you\u0026rsquo;ll get detailed step-by-step instructions for every new chapter/task, later on the guide will become less verbose and we\u0026rsquo;ll weave in some challenges.\nWorkshop Environment As Part of a Red Hat Workshop As part of the workshop you will be provided with freshly installed OpenShift 4.9 clusters. Depending on attendee numbers we might ask you to gather in teams. Some workshop tasks must be done only once for the cluster (e.g. installing Operators), others like deploying and securing the application can be done by every team member separately in their own Project. This will be mentioned in the guide.\nOn Your Own As there is not special setup for the OpenShift cluster you should be able to run the workshop with any 4.9 cluster of you own. Just make sure you have cluster admin privileges.\nThis workshop was tested with these versions :\n OpenShift : 4.9 ACS : 3.68.1 CRW : 2.15.2 Pipelines : 1.6.2 GitOps : 1.4.3  Workshop Flow We\u0026rsquo;ll tackle the topics at hand step by step with an introduction covering the things worked on before every section.\n"
},
{
	"uri": "https://devsecops-workshop.github.io/2-prepare-cluster/",
	"title": "Prepare Cluster",
	"tags": [],
	"description": "",
	"content": "Cluster Preparation Before you start you have to install a number of components you\u0026rsquo;ll use during the workshop. The first two are Gitea for providing Git services in your cluster and CodeReady Workspaces as development environment. But fear not, both are managed by Kubernetes operators on OpenShift.\nInstall and Prepare Gitea We\u0026rsquo;ll need Git repository services to keep our app and infrastructure source code, so let\u0026rsquo;s just install trusted Gitea using an operator:\nGitea is an OpenSource Git Server similar to GitHub. A team at Red Hat was so nice to create an Operator for it. This is a good example of how you can integrate an operator into your catalog that is not part of the default OperatorHub already.\n  If you don\u0026rsquo;t already have the oc client installed, you can download the matching version for your operating system here Log into your OpenShift Webconsole with you cluster admin credentials On the top right click on your username and then Copy login command to copy your login token On you local machine open a terminal log in with the oc command you copied Now using oc add the Gitea Operator to your OpenShift OperatorHub catalog  oc apply -f https://raw.githubusercontent.com/redhat-gpte-devopsautomation/gitea-operator/master/catalog_source.yaml  In the Web Console, go to Operators \u0026gt; OperatorHub and search for Gitea (You may need to disable search filters) Install the Gitea Operator with default settings Create a new OpenShift project called git Go to Installed Operators \u0026gt; Gitea Operator and click on the Create Instance tile in the git project    Click image to enlarge    On the Create Gitea page switch to the YAML view and make sure the following spec values are set:  spec: giteaSsl: true giteaAdminUser: gitea giteaAdminPassword: \u0026quot;gitea\u0026quot; giteaAdminEmail: opentlc-mgr@redhat.com  Click Create  After creation has finished:\n Access the route URL (you\u0026rsquo;ll find it e.g. in Networking \u0026gt; Routes \u0026gt; repository \u0026gt; Location) This will take you to the Gitea web UI Sign-In to Gitea with user gitea and password gitea Clone the example repo:  Click the + dropdown and choose New Migration As type choose Git URL: https://github.com/devsecops-workshop/quarkus-build-options.git Click Migrate Repository    In the cloned repository you\u0026rsquo;ll find a devfile.yml. We will need the URL to the file soon, so keep the tab open.\nInstall and Prepare CodeReady Workspaces (CRW)  Install the Red Hat CodeReady Workspaces for Devfile v1 and v2 Operator from OperatorHub (not the Tech Preview one!) with default settings Go to Installed Operators -\u0026gt; CodeReady Workspaces and create a new instance (CodeReady Workspaces instance Specification) using the default settings in the project openshift-workspaces Wait until deployment has finished. This may take a couple of minutes as several components will be deployed. Once the instance status is ready (You can check the YAML of the instance: status \u0026gt; cheClusterRunning: Available), look up the codeready Route in the openshift-workspaces namespace (You may need to toggle the Show default project button). Open the link in a new browser tab, choose htpasswd_provider and log in with your OCP credentials Allow selected permissions Enter an email address, First Name and Last Name to set up your account (you can make these up).  We could create a workspace from one of the templates that come with CodeReady Workspaces, but we want to use a customized workspace with some additionally defined plugins in a v1 devfile in our git repo. With devfiles you can share a complete workspace setup and with the click of a button you will end up in a fully configured project in your browser.\n  At the top click on Custom Workspace Copy the raw(!) URL of the devfile.yml file in your Gitea repository by clicking on the file and then on the Raw button (or Originalversion in German). Paste the full URL into the Enter devfile URL field and click Load Devfile  Once the content of the devfile is loaded click on Create \u0026amp; Open at the button You\u0026rsquo;ll get into the Starting workspace \u0026hellip; view, give the workspace containers some time to spin up.  When your workspace has finally started, have a good look around in the UI. It should look familiar if you have ever worked with VSCode or similar IDEs.\nWhen working with CRW make sure you have AdBlockers disabled, you are not on a VPN and a have good internet connection to ensure a stable setup. If you are facing any issues try to releod the Browser window. If that doesn\u0026rsquo;t help restart the workspace in the controls on yellow arrow at the top left side\n Your cluster is now prepared for the next step, proceed to the Inner Loop.\n"
},
{
	"uri": "https://devsecops-workshop.github.io/3-inner-loop/",
	"title": "Inner Loop",
	"tags": [],
	"description": "",
	"content": "In this part of the workshop you\u0026rsquo;ll experience how modern software development using the OpenShift tooling can be done in a fast, iterative way. Inner loop here means this is the way, sorry, process, for developers to try out new things and quickly change and test their code on OpenShift without having to build new images all the time or being a Kubernetes expert. Clone the Quarkus Application Code As an example you\u0026rsquo;ll create a new Java application. You don\u0026rsquo;t need to have prior experience programming in Java as this will be kept really simple.\nWe will use a Java application based on the Quarkus stack. Quarkus enables you to create much smaller and faster containerized Java applications than ever before. You can even transcompile these apps to native Linux binaries that start blazingly fast. The app that we will use is just a starter sample created with the Quarkus Generator with a simple RESTful API that answers to http Requests. But at the end of the day this setup will work with any Java application. Fun fact: Every OpenShift Subscription already comes with a Quarkus Subscription.\n Let\u0026rsquo;s clone our project into our workspace :\n Bring up your CodeReady Workspaces in your browser In the bottom left click on Clone Repository and then enter the Git URL to your Gitea Repo (You can copy the URL by clicking on the clipboard icon) Press enter and select the default location.  You should be greeted by the README.md file.\nInstall odo To install the odo cli into your workspace, run the following steps:\n From the CRW shortcuts menu to the right (the \u0026ldquo;cube icon\u0026rdquo;) run install odo    Click image to enlarge    odo cli will be downloaded and unpacked in your project folder, you can safely ignore the warning about file or directory not found Close the install odo terminal tab the installation opened  Login to OpenShift and Create the Development Stage Project Now we want to create a new OpenShift project for our app:\n Open a terminal  In My Workspace (cube icon) to the right click New Terminal   Copy the oc login command from your OpenShift cluster (At the top right Username \u0026gt; Copy login command) and execute in the terminal to log into the OpenShift cluster Create a new project workshop-dev  oc new-project workshop-dev Use odo to Deploy and Update our Application First use odo (\u0026ldquo;OpenShift Do\u0026rdquo;) to list the programming languages/frameworks it supports\n./odo catalog list components Now initialize a new Quarkus application\n./odo create java-quarkus Make the app accessible via http (create a Route)\n./odo url create workshop-app And finally push the app to OpenShift\n./odo push To test the app:\n In OpenShift open the workshop-dev project and switch to the Developer Console Open the Topology view and click on the top right link of Application icon to display the website of the app Your app should show up as a simple web page. In the RESTEasy JAX-RS section click the @Path endpoint /hello to see the result.  Now for the fun part: Using odo you can just dynamically change your code and push it out again without doing a new image build! No dev magic involved:\n In your CRW Workspace on the left, expand the file tree to open file src/main/java/org/acme/GreetingRessource.java and change the string \u0026ldquo;Hello RESTEasy\u0026rdquo; to \u0026ldquo;Hello Workshop\u0026rdquo; (CRW saves every edit directly. No need to save) Push the code to OpenShift again  ./odo push  And reload the app webpage. Bam! The change should be there in a matter of seconds  "
},
{
	"uri": "https://devsecops-workshop.github.io/4-outer-loop/",
	"title": "Outer Loop",
	"tags": [],
	"description": "",
	"content": "Now that you have seen how a developer can quickly start to code using modern cloud native tooling, it\u0026rsquo;s time to learn how to proceed with the application towards production. The first step is to implement a CI/CD pipeline to automate new builds. Let\u0026rsquo;s call this stage int for integration.\nInstall OpenShift Pipelines To create and run the build pipeline you\u0026rsquo;ll use OpenShift Pipelines based on project Tekton. The first step is to install it:\n Install the Red hat OpenShift Pipelines Operator from OperatorHub with default settings Since the Piplines assets are installed asynchronously it is possible that the Pipline Templates are not yet setup when proceeding immedately to the next step. So now is good time to grab a coffee.\n   Create App Deployment and Build Pipeline After installing the Operator create a new deployment of your game-changing application:\n Create a new OpenShift project workshop-int Switch to the OpenShift Developer Console Click the +Add menu entry to the left and choose Import from Git As Git Repo URL enter the clone URL for the quarkus-build-options repo in your your Gitea instance (There might be a warning about the repo url that you can ignore) As Import Strategy select Builder Image As Builder Image select Java and openjdk-11-el7 / Red Hat OpenJDK 11 (RHEL 7) As Application Name enter workshop-app As Name enter workshop Check Add pipeline  If you don\u0026rsquo;t have the checkbox Add pipeline and get the message There are no pipeline templates available for Java and Deployment combination in the next step then just give it few more minutes and reload the page.\n  Click Create In the main menu left, click on Pipelines and observe how the Tekton Pipeline is created and run.  Create an ImageStream Tag with an Old Image Version Now your build pipeline has been set up and is ready. There is one more step in preparation of the security part of this workshop. We need a way to build and deploy from an older image with some security issues in it. For this we will add another ImageStream Tag in the default Java ImageStream that points to an older version with a known issue in it.\n Using the Administrator view, switch to the project openshift and under Builds click on the ImageStreams Search and open the ImageStream java Switch to YAML view and add the following snippet to the tags: section.  Be careful to keep the needed indentation!    - name: java-old-image annotations: description: Build and run Java applications using Maven and OpenJDK 8. iconClass: icon-rh-openjdk openshift.io/display-name: Red Hat OpenJDK 8 (UBI 8) sampleContextDir: undertow-servlet sampleRepo: \u0026#39;https://github.com/jboss-openshift/openshift-quickstarts\u0026#39; supports: \u0026#39;java:8,java\u0026#39; tags: \u0026#39;builder,java,openjdk\u0026#39; version: \u0026#39;8\u0026#39; from: kind: DockerImage name: \u0026#39;registry.redhat.io/openjdk/openjdk-11-rhel7:1.10-1\u0026#39; generation: 4 importPolicy: {} referencePolicy: type: Local This will add a tag java-old-image that points to an older version of the RHEL Java image. The image and security vulnerabilities can be inspected in the Red Hat Software Catalog here: https://catalog.redhat.com/software/containers/openjdk/openjdk-11-rhel7/5bf57185dd19c775cddc4ce5?tag=1.10-1\u0026amp;push_date=1629294893000\u0026amp;container-tabs=security\n Have a look at version 1.10-1  We will use this tag to test our security setup in a later chapter.\n"
},
{
	"uri": "https://devsecops-workshop.github.io/5-gitops/",
	"title": "Configure GitOps",
	"tags": [],
	"description": "",
	"content": "Now that our CI/CD build and integration stage is ready we could promote the app version directly to a production stage. But with the help of the GitOps approach, we can leverage our Git System to handle promotion that is tracked through commits and can deploy and configure the whole production environment. This stage is just too critical to configure manually and without audit.\nInstall OpenShift GitOps So let\u0026rsquo;s start be installing the OpenShift GitOps Operator based on project ArgoCD.\n Install the Red Hat OpenShift GitOps Operator from OperatorHub with default settings The installation of the GitOps Operator will give you a clusterwide ArgoCD instance available at the link in the top right menu, but since we want to have an instance to manage just our prod namespaces we will create another ArgoCD in that specific namespace.\n  Create a new OpenShift Project workshop-prod Then in the project workshop-prod click on Installed Operators and then Red Hat OpenShift GitOps. On the ArgoCD \u0026ldquo;tile\u0026rdquo; click on Create instance to create an ArgoCD instance in the workshop-prod project.    Click image to enlarge    Keep the settings as they are and click Create  Prepare the GitOps Config Repository  In Gitea create a New Migration and clone the Config GitOps Repo which will be the repository that contains our GitOps infrastructure components and state The URL is https://github.com/devsecops-workshop/openshift-gitops-getting-started.git  Have quick look at the structure of this project :\napp - contains yamls for the deployment, service and route resources needed by our application. These will be applied to the cluster. There is also a kustomization.yaml defining that kustomize layers will be applied to all yamls\nenvironments/dev - contains the kustomization.yaml which will be modified by our builds with new Image versions. ArgoCD will pick up these changes and trigger new deployments.\nSetup GitOps Project Let\u0026rsquo;s setup the project that tells ArgoCD to watch our config repo and updated resources in the workshop-prod project accordingly.\n Give namespace workshop-prod permissions to pull images from workshop-int  oc policy add-role-to-user \\ system:image-puller system:serviceaccount:workshop-prod:default \\ --namespace=workshop-int  Find the local ArgoCD URL by going to Networking \u0026gt; Routes in namespace workshop-prod Open the ArgoCD website ignoring the certificate warning Don\u0026rsquo;t login with OpenShift but with username and password User is admin and password will be in Secret argocd-cluster Create App  Click the Manage your applications icon on the left Click Create Application Application Name: workshop Project: default SYNC POLICY: Automatic Repository URL: Copy the URL of your config repo from Gitea (It should resemble https://repository-git.apps.{YOUR DOMAIN}.com/gitea/openshift-gitops-getting-started.git) Path: environments/dev Cluster URL: https://kubernetes.default.svc Namespace: workshop-prod Click Create Click on Sync and then Synchronize to manually trigger the first sync    Watch the resources (Deployment, Service, Route) get rolled out to the namespace workshop-prod. Notice we have also scaled our app to 2 pods in the prod stage as we want some HA.\nOur complete prod stage is now configured and controlled though GitOps. But how do we tell ArgoCD that there is a new version of our app to deploy? Well, we will add a step to our build pipeline updating the config repo.\nAs we do not want to modify our original repo file we will use a tool called Kustomize that can add incremental change layers to YAML files. Since ArgoCD permanently watches this repo it will pick up these Kustomize changes.\nIt is also possible to update the repo with a Pull request. Then you have an approval process for your prod deployment.\n Add Kustomize and Git Push Tekton Task Let\u0026rsquo;s add a new custom Tekton task that can update the Image tag via Kustomize after the build an then push the change to out git config repo.\n In the namespace workshop-int switch to the Administrator Perspective and go to Pipelines \u0026gt; Tasks \u0026gt; Create Task Replace the YAML definition with the following and click Create:  apiVersion: tekton.dev/v1beta1 kind: Task metadata: annotations: tekton.dev/pipelines.minVersion: 0.12.1 tekton.dev/tags: git name: git-update-deployment namespace: workshop-int labels: app.kubernetes.io/version: \u0026#39;0.1\u0026#39; operator.tekton.dev/provider-type: community spec: description: This Task can be used to update image digest in a Git repo using kustomize params: - name: GIT_REPOSITORY type: string - name: GIT_USERNAME type: string - name: GIT_PASSWORD type: string - name: CURRENT_IMAGE type: string - name: NEW_IMAGE type: string - name: NEW_DIGEST type: string - name: KUSTOMIZATION_PATH type: string results: - description: The commit SHA name: commit steps: - image: \u0026#39;docker.io/alpine/git:v2.26.2\u0026#39; name: git-clone resources: {} script: | rm -rf git-update-digest-workdir git clone $(params.GIT_REPOSITORY) git-update-digest-workdir workingDir: $(workspaces.workspace.path) - image: \u0026#39;quay.io/wpernath/kustomize-ubi:latest\u0026#39; name: update-digest resources: {} script: \u0026gt; #!/usr/bin/env bash echo \u0026#34;Start\u0026#34; pwd cd git-update-digest-workdir/$(params.KUSTOMIZATION_PATH) pwd #echo \u0026#34;kustomize edit set image #$(params.CURRENT_IMAGE)=$(params.NEW_IMAGE)@$(params.NEW_DIGEST)\u0026#34; kustomize version kustomize edit set image $(params.CURRENT_IMAGE)=$(params.NEW_IMAGE)@$(params.NEW_DIGEST) echo \u0026#34;##########################\u0026#34; echo \u0026#34;### kustomization.yaml ###\u0026#34; echo \u0026#34;##########################\u0026#34; ls cat kustomization.yaml workingDir: $(workspaces.workspace.path) - image: \u0026#39;docker.io/alpine/git:v2.26.2\u0026#39; name: git-commit resources: {} script: \u0026gt; pwd cd git-update-digest-workdir git config user.email \u0026#34;tekton-pipelines-ci@redhat.com\u0026#34; git config user.name \u0026#34;tekton-pipelines-ci\u0026#34; git status git add $(params.KUSTOMIZATION_PATH)/kustomization.yaml # git commit -m \u0026#34;[$(context.pipelineRun.name)] Image digest updated\u0026#34; git status git commit -m \u0026#34;[ci] Image digest updated\u0026#34; git status git push RESULT_SHA=\u0026#34;$(git rev-parse HEAD | tr -d \u0026#39;\\n\u0026#39;)\u0026#34; EXIT_CODE=\u0026#34;$?\u0026#34; if [ \u0026#34;$EXIT_CODE\u0026#34; != 0 ] then exit $EXIT_CODE fi # Make sure we don\u0026#39;t add a trailing newline to the result! echo -n \u0026#34;$RESULT_SHA\u0026#34; \u0026gt; $(results.commit.path) workingDir: $(workspaces.workspace.path) workspaces: - description: The workspace consisting of maven project. name: workspace Add Tekton Task to Your Pipeline  Go to Pipeline \u0026gt; Pipelines \u0026gt; workshop and then YAML  You can edit pipelines either directly in YAML or in the visual Pipeline Builder. We will see how to use the Builder later on so let\u0026rsquo;s edit the YAML for now.\n Add the new Task to your Pipeline by adding it to the YAML like this:\n In the YAML view insert it at the tasks level after the deploy task For the param GIT_REPOSITORY use your git config repo url (eg. replace {YOUR DOMAIN}) Make sure to fix indentation after pasting into the YAML!  In the OpenShift YAML viewer/editor you can mark multiple lines and use tab to indent this lines for one step.\n - name: git-update-deployment params: - name: GIT_REPOSITORY value: \u0026gt;- https://repository-git.apps.{YOUR DOMAIN}/gitea/openshift-gitops-getting-started.git - name: GIT_USERNAME value: gitea - name: GIT_PASSWORD value: gitea - name: CURRENT_IMAGE value: \u0026gt;- image-registry.openshift-image-registry.svc:5000/workshop-int/workshop:latest - name: NEW_IMAGE value: \u0026gt;- image-registry.openshift-image-registry.svc:5000/workshop-int/workshop - name: NEW_DIGEST value: $(tasks.build.results.IMAGE_DIGEST) - name: KUSTOMIZATION_PATH value: environments/dev runAfter: - build taskRef: kind: Task name: git-update-deployment workspaces: - name: workspace workspace: workspace The Pipeline should now look like this\n  Click image to enlarge    Create a secret with credentials for your Gitea repository, so the task can authenticate and push to Gitea. Replace {YOUR DOMAIN} here to match your GiteaURL You can add this by clicking on the + on the top right ob the Web Console  kind: Secret apiVersion: v1 metadata: name: gitea namespace: workshop-int annotations: tekton.dev/git-0: \u0026#39;https://repository-git.apps.{YOUR DOMAIN}\u0026#39; data: password: Z2l0ZWE= username: Z2l0ZWE= type: kubernetes.io/basic-auth Now we need to add the secret to the serviceaccount that runs our pipelines so the task can push to our config repo.\n Go to User Management \u0026gt; ServiceAccounts \u0026gt; pipeline To make the secret available during a pipeline run: Open the YAML and in the secrets section add:  - name: gitea  Save and ignore the warning  Update our Prod Stage via Pipeline and GitOps   Run the pipeline and see that in your Gitea repo /environment/dev/kustomize.yaml is updated with the new image version Notice that the deploy and the git-update steps now run in parallel. This is one of the powers of Tekton. It can scale natively with pods on OpenShift.\n   This will tell ArgoCD to update the Deployment with this new image version\n  Check that the new image is rolled out (you may need to sync manually in ArgoCD to speed things up)\n  "
},
{
	"uri": "https://devsecops-workshop.github.io/10-rhacs-setup/",
	"title": "Install and Configure ACS",
	"tags": [],
	"description": "",
	"content": "During the workshop you went through the OpenShift developer experience starting from software development using Quarkus and odo, moving on to automating build and deployment using Tekton pipelines and finally using GitOps for production deployments.\nNow it\u0026rsquo;s time to add another extremely important piece to the setup; enhancing application security in a containerized world. Using a recent addition to the OpenShift portfolio: Red Hat Advanced Cluster Security for Kubernetes!\nInstall RHACS Install the Operator Install the \u0026ldquo;Advanced Cluster Security for Kubernetes\u0026rdquo; operator from OperatorHub:\n Switch Update approval to Manual Apart from this use the default settings Approve the installation when asked  Red Hat recommends installing the Red Hat Advanced Cluster Security for Kubernetes Operator in the rhacs-operators namespace. This will happen by default..\n Install the main component Central You must install the ACS Central instance in its own project and not in the rhacs-operator and openshift-operator projects, or in any project in which you have installed the ACS Operator!\n  Navigate to Operators → Installed Operators Select the ACS operator You should now be in the rhacs-operator project the Operator created, create a new OpenShift Project for the Central instance:  Select Project: rhacs-operator → Create project Create a new project called stackrox (Red Hat recommends using stackrox as the project name.)   In the Operator view under Provided APIs on the tile Central click Create Instance Accept the name stackrox-central-services Adjust the memory limit of the central instance to 6Gi (Central Component Settings-\u0026gt;Resources-\u0026gt;Limits\u0026gt;Memory). Click Create  After deployment has finished (Status Conditions: Deployed, Initialized in the Operator view on the tab Central) it can take some time until the application is completely up and running. One easy way to check the state is to switch to the Developer console view at the upper left. Then make sure you are in the stackrox project and open the Topology map. You\u0026rsquo;ll see the three deployments of an Central instance:\n scanner-db scanner centrals  Wait until all Pods have been scaled up properly.\nVerify the Installation\nSwitch to the Administrator console view again. Now to check the installation of your Central instance, access the ACS Portal:\n Look up the central-htpasswd secret that was created to get the password  If you access the details of your Central instance in the Operator page you\u0026rsquo;ll find the complete commandline using oc to retrieve the password from the secret under Admin Credentials Info. Just sayin\u0026hellip; ;)\n  Look up and access the route central which was also generated automatically.  This will get you to the ACS Portal, accept the self-signed certificate and login as user admin with the password from the secret.\nNow you have a Central instance that provides the following services in an RHACS setup:\n  The application management interface and services. It handles data persistence, API interactions, and user interface access. You can use the same Central instance to secure multiple OpenShift or Kubernetes clusters.\n  Scanner, which is a vulnerability scanner for scanning container images. It analyzes all image layers to check known vulnerabilities from the Common Vulnerabilities and Exposures (CVEs) list. Scanner also identifies vulnerabilities in packages installed by package managers and in dependencies for multiple programming languages.\n  To actually do and see anything you need to add a SecuredCluster (be it the same or another OpenShift cluster). For effect go to the ACS Portal, the Dashboard should by pretty empty, click on the Compliance link in the menu to the left, lots of zero\u0026rsquo;s and empty panels, too.\nThis is because you don\u0026rsquo;t have a monitored and secured OpenShift cluster yet.\nPrepare to add Secured Clusters First you have to generate an init bundle which contains certificates and is used to authenticate a SecuredCluster to the Central instance, again regardless if it\u0026rsquo;s the same cluster as the Central instance or a remote/other cluster.\nIn the ACS Portal:\n Navigate to Platform Configuration → Integrations. Under the Authentication Tokens section, click on Cluster Init Bundle. Click Generate bundle Enter a name for the cluster init bundle and click Generate. Click Download Kubernetes Secret File to download the generated bundle.  The init bundle needs to be applied on all OpenShift clusters you want to secure \u0026amp; monitor.\nPrepare the Secured Cluster For this workshop we run Central and SecuredCluster on one OpenShift cluster. E.g. we monitor and secure the same cluster the central services live on.\nApply the init bundle\n Use the oc command to log in to the OpenShift cluster as cluster-admin.  The easiest way might be to use the Copy login command link from the UI   Switch to the Project you installed ACS Central in, it should be stackrox. Run oc create -f \u0026lt;init_bundle\u0026gt;.yaml -n stackrox pointing to the init bundle you downloaded from the Central instance and the Project you created. This will create a number of secrets:  secret/collector-tls created secret/sensor-tls created secret/admission-control-tls created Add the Cluster as SecuredCluster to ACS Central You are ready to install the SecuredClusters instance, this will deploy the secured cluster services:\n In the OpenShift Web Console go to the ACS Operator in Operators-\u0026gt;Installed Operators Using the Operator create an instance of the Secured Cluster type in the Project you created (should be stackrox) Change the Cluster Name for the cluster if you want, it\u0026rsquo;ll appear under this name in the ACS Portal And most importantly for Central Endpoint enter the address and port number of your Central instance, this is the same as the ACS Portal.  If the ACS Portal is available at https://central-stackrox.apps.cluster-65h4j.65h4j.sandbox1803.opentlc.com/ the endpoint is central-stackrox.apps.cluster-65h4j.65h4j.sandbox1803.opentlc.com:443.   Under Admission Control Settings make sure  listenOnCreates, listenOnUpdates and Listen On Events is enabled Set Contact Image Scanners to ScanIfMissing   Click Create  Now go to your ACS Portal again, after a couple of minutes you should see you secured cluster under Platform Configuration-\u0026gt;Clusters. Wait until all Cluster Status indicators become green.\n"
},
{
	"uri": "https://devsecops-workshop.github.io/11-rhacs-warmup/",
	"title": "Getting to know ACS",
	"tags": [],
	"description": "",
	"content": "Before we start to integrate Red Hat Advanced Cluster Security in our setup, you should become familiar with the basic concepts.\nACS Features ACS delivers on these security use cases:\n Vulnerability Management: Protect the software supply chain and prevent known vulnerabilities from being used as an entry point in your applications. Configuration Management: Leverage the OpenShift platform for declarative security to prevent or limit attacks, even in the presence of exploitable vulnerabilities. Network Segmentation: Using Kubernetes network policies in OpenShift, restrict open network paths for isolation and to prevent lateral movement by attackers. Risk Profiling: Prioritize applications and security risks automatically to focus investigation and mitigation efforts. Threat detection and incident response: Continuous observation and response in order to take action on attack-related activity, and to use observed behavior to inform mitigation efforts to harden security. Compliance: Making sure that industry and regulatory standards are being met in your OpenShift environments.  UI Overview   Click image to enlarge     Dashboard: The dashboard serves as the security overview - helping the security team understand what the sources of risk are, categories of violations, and gaps in compliance. All of the elements are clickable for more information and categories are customizable.\n  Top bar: Near the top, we see an overview of our OpenShift clusters. It provides insight into the usage of images and secrets. The top bar provides links to Search, Command-line tools, Cluster Health, Documentation, API Reference, and logged-in user account\n  Left menus: The left hands side menus provide navigation into each of the security use-cases, as well as product configuration to integrate with your existing tooling.\n  Global Search: On every page throughout the UI, the global search allows you to search for any data that ACS tracks.\n  Exploring the Security Use Cases Now start to explore the Security Use Cases ACS targets as provided in the left side menu.\n  Network Graph:\n The Network Graph is a flow diagram, firewall diagram, and firewall rule builder in one. The default view Active the actual traffic for the Past Hour between the deployments in all namespaces is shown.    Violations:\n Violations record all times where a policy criteria was triggered by any of the objects in your cluster - images, components, deployments, runtime activity.    Compliance:\n The compliance reports gather information for configuration, industry standards, and best practices for container-based workloads running in OpenShift.    Vulnerability Management:\n Vulnerability Management provides several important reports - where the vulnerabilities are, which are the most widespread or the most recent, where my images are coming from. In the upper right are buttons to link to all policies, CVEs, and images, and a menu to bring you to reports by cluster, namespace, deployment, and co.    Configuration Management:\n Configuration management provides visibility into a number of infrastructure components: clusters and nodes, namespaces and deployments, and Kubernetes systems like RBAC and secrets.    Risk:\n The Risk view goes beyond the basics of vulnerabilities. It helps to understand how deployment configuration and runtime activity impact the likelihood of an exploit occurring and how successful those exploits will be. This list view shows all deployments, in all clusters and namespaces, ordered by Risk priority.    Filters Most UI pages have a filters section at the top that allows you to narrow the view to matching or non-matching criteria. Almost all of the attributes that ACS gathers are filterable, try it out:\n Go to the Risk view Click in the Filters Bar Start typing Process Name and select the Process Name key Type java and press enter; click away to get the filters dropdown to clear You should see your deployment that has been “seen” running Java since it started Try another one: limit the filter to your Project namespace only Note the Create Policy button. It can be used to create a policy from the search filter to automatically identify this criteria.  System Policies As the foundation of ACS are the system policies, have a good look around:\n Navigate to the Policies section from Platform Configuration in the left side menu. You will get an overview of the Built-in Policies All of the policies that ship with the product are designed with the goal of providing targeted remediation that improves security hardening. You’ll see this list contains many Build and Deploy time policies to catch misconfigurations early in the pipeline, but also Runtime policies. These policies come from us at Red Hat - our expertise, our interpretation of industry best practice, and our interpretation of common compliance standards, but you can modify them or create your own.  By default only some policies are enforced. If you want to get an overview which ones, you can use the filter view introduced above. Use Enforcement as filter key and FAIL_BUILD_ENFORCEMENT as value.\n "
},
{
	"uri": "https://devsecops-workshop.github.io/12-create-policy/",
	"title": "Create a Custom Security Policy",
	"tags": [],
	"description": "",
	"content": "Objective You should have one or more pipelines to build your application from the first workshop part, now we want to secure the build and deployment of it. For the sake of this workshop we\u0026rsquo;ll take a somewhat simplified use case:\nWe want to scan our application image for the Red Hat Security Advisory RHSA-2020:5566 concerning openssl-lib.\nIf this RHSA is found in an image we don\u0026rsquo;t want to deploy the application using it.\nThese are the steps you will go through:\n Create a custom Security Policy to check for the advisory Test if the policy is triggered in non-enforcing mode  with an older image version that contains the issue and then with a newer version with the issue fixed.   The final goal is to integrate the policy into the build pipeline  Create a Custom System Policy First create the system policy. In the ACS Portal do the following:\n Platform Configuration-\u0026gt;Policies-\u0026gt;Create policy Policy Details  Name: Workshop RHSA-2021:4904 Severity: Critical Categories: Workshop  This will create a new Category if it doesn\u0026rsquo;t exist   Click Next   Policy Behaviour  Lifecycle Stages: Build, Deploy Response method: Inform Click Next   Policy Criteria  Find the CVE policy criterium under Drag out policy fields in Image contents Drag \u0026amp; drop it on the drop zone of Policy Section 1 Put RHSA-2021:4904 into the CVE identifier field Click Next   Policy Scope  You could limit the scope the policy is applied in, do nothing for now.   Review Policy  Have a quick look around, if the policy would create a violation you get a preview here. Click Save      Click image to enlarge   Test the Policy Start the pipeline with the affected image version:\n In the OpenShift Web Console go to the Pipeline in your workshop-int project, start it and set Version to java-old-image (Remember how we set up this ImageStream tag to point to an old and vulnerable version of the image?) Follow the Violations in the ACS Portal Expected result:  You\u0026rsquo;ll see the build deployments (Quarkus-Build-Options-Git-Gsklhg-Build-...) come and go when they are finished. When the final build is deployed you\u0026rsquo;ll see a violation in ACS Portal for policy Workshop RHSA-2021:4904 (Check the Time of the violation)    There will be other policy violations listed, triggered by default policies, have a look around. Note that none of the policies is enforced (as in stop the pipeline build) yet!\n Now start the pipeline with the fixed image version that doesn\u0026rsquo;t contain the CVE anymore:\n Start the pipeline again but this time leave the Java Version as is (openjdk-11-el7). Follow the Violations in the ACS Portal Expected result:  You\u0026rsquo;ll see the build deployments come up and go When the final build is deployed you\u0026rsquo;ll see the policy violation for Workshop RHSA-2021:4904 for your deployment is gone because the image no longer contains it.    This shows how ACS is automatically scanning images when they become active against all enabled policies. But we don\u0026rsquo;t want to just admire a violation after the image has been deployed, we want to disable the deployment during build time! So the next step is to integrate the check into the build pipeline and enforce it (don\u0026rsquo;t deploy the application).\n"
},
{
	"uri": "https://devsecops-workshop.github.io/13-rhacs-pipeline/",
	"title": "Integrating ACS into the Pipeline",
	"tags": [],
	"description": "",
	"content": "Finally: Putting the Sec in DevSecOps! There are basically two ways to interface with ACS. The UI, which focuses on the needs of the security team, and a separate \u0026ldquo;interface\u0026rdquo; for developers to integrate into their existing toolset (CI/CD pipeline, consoles, ticketing systems etc): The roxctl commandline tool. This way ACS provides a familiar interface to understand and address issues that the security team considers important.\n ACS policies can act during the CI/CD pipeline to identify security risk in images before they are run as a container.\nIntegrate Image Scan into the Pipeline You should have created and build a custom policy in ACS and tested it for triggering violations. Now you will integrate it into the build pipeline.\nLet\u0026rsquo;s go: Prepare roxctl Build-time policies require the use of the roxctl command-line tool which is available for download from the ACS Central UI, in the upper right corner of the dashboard. Roxctl needs to authenticate to ACS Central to do anything. It can use either username and password API tokens to authenticate against Central. It\u0026rsquo;s good practice to use a token so that\u0026rsquo;s what we\u0026rsquo;ll do.\nCreate the roxctl token In the ACS portal:\n Navigate to Platform Configure \u0026gt; Integrations. Scroll down to the Authentication Tokens category, and select API Token. Click Generate Token. Enter the name pipeline for the token and select the role Admin. Select Generate Save the contents of the token somewhere!  Create OCP secret with token Change to the OpenShift Web Console and create a secret with the API token in the project your pipeline lives in:\n In the UI switch to your Project Create a new key/value Secret named roxsecrets Introduce these key/values into the secret:  rox_central_endpoint: \u0026lt;the URL to your ACS Portal\u0026gt;  It should be something like central-stackrox.apps.cluster-psslb.psslb.sandbox555.opentlc.com:443   rox_api_token: \u0026lt;the API token you generated\u0026gt;    Create a Scan Task You are now ready to create a new pipeline task that will use roxctl to scan the image build in your pipeline before the deploy step:\n In the OCP UI, make sure you are still in the project with your pipeline and the secret roxsecrets Go to Pipelines-\u0026gt;Tasks Click Create-\u0026gt; ClusterTask Replace the YAML displayed with this:  apiVersion: tekton.dev/v1beta1 kind: ClusterTask metadata: name: rox-image-check spec: params: - description: \u0026gt;- Secret containing the address:port tuple for StackRox Central (example - rox.stackrox.io:443) name: rox_central_endpoint type: string - description: Secret containing the StackRox API token with CI permissions name: rox_api_token type: string - description: \u0026#39;Full name of image to scan (example -- gcr.io/rox/sample:5.0-rc1)\u0026#39; name: image type: string - description: Use image digest result from s2i-java build task name: image_digest type: string results: - description: Output of `roxctl image check` name: check_output steps: - env: - name: ROX_API_TOKEN valueFrom: secretKeyRef: key: rox_api_token name: $(params.rox_api_token) - name: ROX_CENTRAL_ENDPOINT valueFrom: secretKeyRef: key: rox_central_endpoint name: $(params.rox_central_endpoint) image: registry.access.redhat.com/ubi8/ubi-minimal:latest name: rox-image-check resources: {} script: \u0026gt; #!/usr/bin/env bash set +x curl -k -L -H \u0026#34;Authorization: Bearer $ROX_API_TOKEN\u0026#34; https://$ROX_CENTRAL_ENDPOINT/api/cli/download/roxctl-linux --output ./roxctl \u0026gt; /dev/null; echo \u0026#34;Getting roxctl\u0026#34; chmod +x ./roxctl \u0026gt; /dev/null ./roxctl image check -c Workshop --insecure-skip-tls-verify -e $ROX_CENTRAL_ENDPOINT --image $(params.image)@$(params.image_digest) Take your time to understand the Tekton task definition:\n First some parameters are defined, it\u0026rsquo;s important to understand some of these are taken or depend on the build task that run before. The script action pulls the roxctl binary into the pipeline workspace so you\u0026rsquo;ll always have a version compatible with your ACS version. The most important bit is the roxctl execution, of course.  it executes the image check command only checks against policies from category Workshop that was created above. This way you can check against a subset of policies! defines the image to check and it\u0026rsquo;s digest    Add the Task to the Pipeline  Now add the rox-image-check task to your pipeline between the build and deploy steps.  After you added it you have to fill in values for the parameters the task defines. Click the task, a form with the parameters will open, fill it in:\n rox_central_endpoint: roxsecrets rox_api_token: roxsecrets image: image-registry.openshift-image-registry.svc:5000/workshop-int/workshop  Adapt the Project name if you changed it   image_digest: $(tasks.build.results.IMAGE_DIGEST)  This variable takes the result of the build task and uses it in the scan task.   Click Save    Click image to enlarge   Test the Scan Task With our custom System Policy still not set to enforce we first are going to test the pipeline integration. Start the pipeline with Java Version java-old-image\n Expected Result:  The rox-image-check task should succeed, but if you have a look at the output (click the task in the visual representation) you should see that the build violated our policy!    To test the fixed image, just start the task with the default (latest) Java version again.\n Expected Result:  The rox-image-check task should succeed, if you have a look at the output you should see no policy violation!    Enforce the Policy The last step is to enforce the System Policy. If the policy is violated the pipeline should be stopped and the application should not be deployed.\n Edit the System Policy in ACS Portal and set enforcement to On for the stages Build and Deploy Run the pipeline again, first with Java Version java-old-image and then with the latest default version. Expecte results:  We are sure you know by now what to expect! The pipeline should fail with the old Java image version and succeed with the latest image version!    "
},
{
	"uri": "https://devsecops-workshop.github.io/14-advanced-pipeline/",
	"title": "Advanced ACS Pipeline",
	"tags": [],
	"description": "",
	"content": "Optimizing our DevSecOps Pipeline In the chapter before we created a Pipeline which builds, checks and deploys our image, if the image passes all ACS checks. But what should happen, if the image doesn\u0026rsquo;t pass the ACS check? It doesn\u0026rsquo;t seems to be a good idea to save an unsafe image in the internal OpenShift registry. So lets modify our pipeline to an advanced one, which deletes an image, if it doesn\u0026rsquo;t pass the ACS checks.\nAdding a Package Level Vulnerability Before we adjust our pipeline let\u0026rsquo;s add another vulnerability to our Deployment. So far we had a system library security issue but let\u0026rsquo;s image a developer tried to add (hopefully by mistake) a vulnerable Java library to his/her application. How about we pick one of the most infamous current ones such as the Log4Shell vulnerability? But don\u0026rsquo;t worry ACS already ships with a policy for that and has your back.\nQuarkus doesn\u0026rsquo;t use the log4J library for logging so is not affected by it. Although it is a bit contrived we will still force it to include the library just to trigger our policy. If you want to find out more about this particular vulnerability have look here\n To add this library to you Quarkus app\n Go to your quarkus-build-options repo in Gitea Edit the pom.xml file to include the new dependency Find the line  \u0026lt;!-- Add dependency here --\u0026gt; and after that paste\n\u0026lt;!-- Add dependency here --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.9.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.9.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Commit Edit the src/main/java/org/acme/GreetingResource.java file to use the new dependency (otherwise Quarkus will just optimize it away) Find the line  // Add import here and after that paste\n// Add import here  import org.apache.logging.log4j.Logger; import org.apache.logging.log4j.LogManager;  Find the line  // Add Logger instantiation here and after that paste\n// Add Logger instantiation here  private Logger logger = LogManager.getLogger(GreetingResource.class.getName());  Commit  There you go. That developer has just introduced a ticking timebomb into the application. Let\u0026rsquo;s see how far he will get with that.\nACS Prerequisites So there is a major security vulnerability in our code. ACS would detect the deployment because the System Policy Log4Shell: log4j Remote Code Execution vulnerability is enabled but won\u0026rsquo;t stop it, because the Policy is not set to enforce.\nModify Log4Shell Policy So first of all in the ACS Portal follow these steps:\n Navigate to Platform Configuration \u0026gt; System Policies Search for Log4Shell and click on the Policy. Click Edit at the upper right  Remember setting up image scanning with roxctl in our first pipeline? We use roxctl in a way that it only scans against Policies from a certain category, Workshop in our case. To have the task scan for the Log4Shell Policy, first add the Policy to the category Workshop:\n Remove the current categorie and add Workshop Now hit next until you are at the enforcement tab Enable both Build and Deploy enforcement by setting them to On Save the policy  ACS is now able to detect and enforce the vulnerability. It is time now to implement your advanced Pipeline.\nLet\u0026rsquo;s go: Create our advanced Pipeline In the OpenShift Web Console:\n Make sure you are in the workshop-int Project Navigate to Pipelines \u0026gt; Pipelines On the right click Create \u0026gt; Pipeline Switch to YAML view Replace the YAML with this making sure to update your two Gitea Repo URL\u0026rsquo;s (Specifically replace the {YOUR_CLUSTER_HOSTNAME}):  apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: labels: name: workshop-advanced namespace: workshop-int spec: finally: - name: delete-image params: - name: SCRIPT value: \u0026#39;oc tag -d workshop-int/workshop:dev -n workshop-int\u0026#39; - name: VERSION value: latest taskRef: kind: ClusterTask name: openshift-client when: - input: $(tasks.image-check.status) operator: in values: - Failed params: - default: workshop name: APP_NAME type: string - default: \u0026gt;- https://repository-git.apps.{YOUR_CLUSTER_HOSTNAME}/gitea/quarkus-build-options.git name: GIT_REPO type: string - default: master name: GIT_REVISION type: string - default: \u0026gt;- image-registry.openshift-image-registry.svc:5000/workshop-int/workshop:dev name: IMAGE_NAME type: string - default: . name: PATH_CONTEXT type: string - default: openjdk-11-el7 name: VERSION type: string tasks: - name: git-update-deployment params: - name: GIT_REPOSITORY value: \u0026gt;- https://repository-git.apps.{YOUR_CLUSTER_HOSTNAME}/gitea/openshift-gitops-getting-started.git - name: GIT_USERNAME value: gitea - name: GIT_PASSWORD value: gitea - name: CURRENT_IMAGE value: \u0026gt;- image-registry.openshift-image-registry.svc:5000/workshop-int/workshop:latest - name: NEW_IMAGE value: \u0026gt;- image-registry.openshift-image-registry.svc:5000/workshop-int/workshop - name: NEW_DIGEST value: $(tasks.build.results.IMAGE_DIGEST) - name: KUSTOMIZATION_PATH value: environments/dev runAfter: - remove-dev-tag taskRef: kind: Task name: git-update-deployment workspaces: - name: workspace workspace: workspace - name: fetch-repository params: - name: url value: $(params.GIT_REPO) - name: revision value: $(params.GIT_REVISION) - name: deleteExisting value: \u0026#39;true\u0026#39; taskRef: kind: ClusterTask name: git-clone workspaces: - name: output workspace: workspace - name: build params: - name: IMAGE value: $(params.IMAGE_NAME) - name: TLSVERIFY value: \u0026#39;false\u0026#39; - name: PATH_CONTEXT value: $(params.PATH_CONTEXT) - name: VERSION value: $(params.VERSION) runAfter: - fetch-repository taskRef: kind: ClusterTask name: s2i-java workspaces: - name: source workspace: workspace - name: deploy params: - name: SCRIPT value: oc rollout status deploy/$(params.APP_NAME) runAfter: - remove-dev-tag taskRef: kind: ClusterTask name: openshift-client - name: image-check params: - name: rox_central_endpoint value: roxsecrets - name: rox_api_token value: roxsecrets - name: image value: \u0026gt;- image-registry.openshift-image-registry.svc:5000/workshop-int/workshop - name: image_digest value: $(tasks.build.results.IMAGE_DIGEST) runAfter: - build taskRef: kind: ClusterTask name: rox-image-check - name: tag-checked-image params: - name: SCRIPT value: \u0026#39;oc tag workshop:dev workshop:latest\u0026#39; - name: VERSION value: latest runAfter: - image-check taskRef: kind: ClusterTask name: openshift-client - name: remove-dev-tag params: - name: SCRIPT value: \u0026#39;oc tag -d workshop:dev\u0026#39; - name: VERSION value: latest runAfter: - tag-checked-image taskRef: kind: ClusterTask name: openshift-client workspaces: - name: workspace  Click Create  As you can see in the Pipeline visualization the flow is now a bit different. Lets see what has changed:\n The image which was built through the build task will be pushed in a new ImageStreamTag called dev If the image passes the ACS checks, the image will be pushed to the ImageStreamTag called latest and the dev tag will be removed If the image doesn\u0026rsquo;t pass the ACS checks, the ImageStreamTag called dev will be removed. This is handled by a function called finally and a when expression  Have a look at your Pipeline YAML and try to understand how it works   The last three steps stays the same as before    Click image to enlarge   Test the advanced Pipeline Go ahead and start your newly created advanced Pipeline. Navigate to:\n Pipelines -\u0026gt; Pipelines Click on workshop-advanced In the top right corner click Actions -\u0026gt; Start In the Start Pipeline window at the bottom set workspace to PersistentVolumeClaim Set the Select a PVC drop-down to a PVC Start the Pipeline  See what happens and maybe play around with it and start again. Have fun!\nFix the Vulnerability If by now the developer that introduced the log4jShell vulnerability has not realized that he/she \u0026ldquo;broke the build\u0026rdquo; you can tell him/her to update their dependency to a safe version.\nGo to your quarkus-build-options repo in Gitea again\n Edit the pom.xml file update dependency to a safe version like this  \u0026lt;!-- Add dependency here --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.17.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.17.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; "
},
{
	"uri": "https://devsecops-workshop.github.io/15-runtime-security/",
	"title": "Securing Runtime Events",
	"tags": [],
	"description": "",
	"content": "So far you\u0026rsquo;ve seen how ACS can handle security issues concerning Build and Deploy stages. But ACS is also able to detect and secure container runtime behaviour. Let\u0026rsquo;s have a look\u0026hellip;\nHandling Security Issues at Runtime As a scenario let\u0026rsquo;s assume you want to protect container workloads against attackers who are trying to install software. ACS comes with pre-configured policies for Ubuntu and Red Hat-based containers to detect if a package management tool is installed, this can be used in the Build and Deploy stages:\n Red Hat Package Manager in Image  And, more important for this section about runtime security, a policy to detect the execution of a package manager as a runtime violation, using Kernel instrumentation:\n Red Hat Package Manager Execution  In the ACS Portal, go to Platform Configuration-\u0026gt;System Policies, search for the policies by e.g. typing policy and then red hat into the filter. Open the policy detail view by clicking it and have a look at what they do.\nYou can use the included policies as they are but you can always e.g. clone and adapt them to your needs or write completely new ones.\n As you can see the Red Hat Package Manager Execution policy will alert as soon as a process rpm or dnf or yum is executed.\nLike with most included policies it is not set to enforce!\n Test the Runtime Policy To see how the alert would look like, we have to trigger the condition:\n You should have a namespace with your Quarkus application runnning In the OpenShift Web Console navigate to the pod and open a terminal into the container Run yum search test Go to the Violations view in the ACS Portal. You should see a violation of the policy, if you click it, you\u0026rsquo;ll get the details. Run several yum commands in the terminal and check back with the Violations view:  As long as you stay in the same deployment, there won\u0026rsquo;t be a new violation but you will see the details for every new violation of the same type in the details.    Enforce Runtime Protection But the real fun starts when you enforce the policy. Using the included policy, it\u0026rsquo;s easy to just \u0026ldquo;switch it on\u0026rdquo;:\n In the ACS Portal bring up the Red Hat Package Manager Execution again. Click the Edit button above the Details view to the right. Click -\u0026gt;Next until you arrive at the Enforcement Behaviour page. Set Runtime to ON Click Save  Now trigger the policy again by opening a terminal into the pod in the OpenShift Web Console and executing yum. See what happens:\n Runtime enforcement will kill the pod immediately (via k8s). OpenShift will scale it up again automatically  This is to be expected and allows to contain a potential compromise while not causing a production outage.    "
},
{
	"uri": "https://devsecops-workshop.github.io/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "DevSecOps Workshop What is it about This workshop will introduce you to the application development cycle leveraging OpenShift\u0026rsquo;s tooling \u0026amp; features with a special focus on securing your environment using Advanced Cluster Security for Kubernetes (ACS). You will get a brief introduction in several OpenShift features like OpenShift Pipelines, OpenShift GitOps, OpenShift CodeReadyWorkspaces. And all in a fun way.\nArchitecture overview   Click image to enlarge   Collaboration This workshop was created by Daniel Brintzinger, Goetz Rieger and Sebastian Dehn. Feel free to create a pull request or an issue in GitHub\n"
},
{
	"uri": "https://devsecops-workshop.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://devsecops-workshop.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]